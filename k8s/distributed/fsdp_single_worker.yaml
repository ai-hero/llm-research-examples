compute_environment: LOCAL_MACHINE
debug: false
distributed_type: FSDP
downcast_bf16: 'no'
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP # Autowrap policy https://pytorch.org/docs/stable/fsdp.html#module-torch.distributed.fsdp
  fsdp_backward_prefetch_policy: BACKWARD_PRE  # pytorch explicit param https://pytorch.org/docs/stable/fsdp.html#module-torch.distributed.fsdp
  fsdp_cpu_ram_efficient_loading: true
  fsdp_forward_prefetch: false # pytorch explicit param https://pytorch.org/docs/stable/fsdp.html#module-torch.distributed.fsdp
  fsdp_offload_params: false # https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.CPUOffload
  fsdp_sharding_strategy: 2 # https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.ShardingStrategy
  fsdp_state_dict_type: SHARDED_STATE_DICT
  fsdp_sync_module_states: true # https://pytorch.org/docs/stable/fsdp.html#module-torch.distributed.fsdp
  fsdp_use_orig_params: true # https://pytorch.org/docs/stable/fsdp.html#module-torch.distributed.fsdp
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 2
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
